# POC Drivers

Why each technology and practice was chosen, and what it proves in the context of this POC.


> Open Source & Public Standards

Build on well-known, vendor-neutral protocols and open-source tools — no proprietary lock-in.

| Concern          | Standard / Tool                                                                                                                | Why it matters                                                            |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------- |
| Agent ↔ Frontend | [AG-UI Protocol](https://docs.ag-ui.com/introduction)                                                                          | Open event-based protocol, any frontend can connect                       |
| Observability    | [OpenTelemetry](https://opentelemetry.io/) + [GenAI Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/) | Industry standard for traces, metrics, logs — including AI-specific spans |
| State sync       | [JSON Patch (RFC 6902)](https://datatracker.ietf.org/doc/html/rfc6902)                                                         | Standard for incremental state updates in AG-UI                           |
| Transport        | SSE / HTTP                                                                                                                     | Web-native, no custom protocols                                           |
| API spec         | [OpenAPI](https://www.openapis.org/)                                                                                           | Auto-generated by FastAPI                                                 |
| LLM inference    | [Ollama](https://ollama.com/) (Qwen3 8B)                                                                                       | Local, 32K context, best reasoning at 8B size                             |
| Embeddings       | [sentence-transformers](https://www.sbert.net/) (`snowflake-arctic-embed-s`)                                                   | Retrieval-optimized, 384-dim, MTEB 51.98                                  |
| Vector store     | [Qdrant](https://qdrant.tech/)                                                                                                 | Pre-search filtering, hybrid search (RRF), Aspire integration             |
| Web scraping     | [Crawl4AI](https://github.com/unclecode/crawl4ai)                                                                              | Open-source, LLM-native                                                   |
| Orchestration    | [.NET Aspire](https://aspire.dev/)                                                                                             | Open-source, polyglot app orchestrator                                    |

> Production-Grade Best Practices (where easy to adopt)

Demonstrate real-world engineering quality without over-engineering the POC.

| Practice                       | How we apply it                                                  |
| ------------------------------ | ---------------------------------------------------------------- |
| **Observability from day one** | OTel traces + GenAI semantic conventions → Aspire Dashboard      |
| **Structured orchestration**   | Aspire AppHost: service discovery, health checks, unified logs   |
| **Typed contracts**            | Pydantic models for data schemas, AG-UI typed events             |
| **Source traceability**        | Every chunk tracks origin URL + scrape timestamp                 |
| **Offline-first**              | Local LLM + local embeddings — chat works without internet       |
| **Separation of concerns**     | Gather phase (online) vs. Query phase (offline) are independent  |
| **Modular architecture**       | Swap LLM, vector store, or scraper without touching other layers |
| **Health checks**              | Ollama model readiness, Qdrant availability — via Aspire         |
| **Semantic chunking**          | Context-aware splits, not arbitrary fixed-size windows           |
| **Citation in answers**        | Every response references source documents                       |



## 1. Open Source & Vendor-Neutral Standards

No proprietary lock-in. Every component is replaceable.

### Agent ↔ Frontend: [AG-UI Protocol](https://docs.ag-ui.com/introduction)

**Motivation:** The POC needs a chat interface but building a custom frontend from scratch is out of scope. AG-UI is an open event-based protocol; [CopilotKit](https://docs.copilotkit.ai/) provides production-ready React chat components that speak AG-UI natively. A minimal Next.js app with CopilotKit gives us a clean chat UI in ~8 files.

**What it proves:** The agent backend can serve any AG-UI-compatible frontend — swap CopilotKit for a custom UI later without touching the backend.

### Observability: [OpenTelemetry](https://opentelemetry.io/) + [GenAI Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/)

**Motivation:** A RAG pipeline has many failure points — scraping, chunking, embedding, retrieval, generation. Without observability, debugging "why did the answer cite the wrong source?" is guesswork. OTel GenAI conventions give us standardized spans for LLM calls, tool invocations, and token usage.

**What it proves:** AI-specific observability is practical from day one, not a production afterthought. Traces through the full pipeline (scrape → chunk → embed → retrieve → generate) are visible in one dashboard.

### LLM Inference: [Ollama](https://ollama.com/)

**Motivation:** The spec requires offline chat — no API calls during query phase. Ollama runs local models with a simple API that Pydantic AI supports natively. Swapping models is a config change (`qwen3` → `llama3.1`).

**What it proves:** Local LLM inference is viable for grounded QA on consumer hardware (~5.6 GB VRAM).

**Current model:** Qwen3 8B (Q4_K_M) — 32K context, best reasoning at 8B size. See [tradeoffs/06-llm-inference.md](tradeoffs/06-llm-inference.md).

### Embeddings: [sentence-transformers](https://www.sbert.net/)

**Motivation:** Offline embedding generation is required (no cloud APIs). sentence-transformers is the standard Python library for running embedding models locally. Supports CPU inference, batch processing, and the model we chose.

**What it proves:** Local embedding generation is fast enough for the POC data volume (<100K chunks, <500ms per chunk on CPU).

**Current model:** `snowflake-arctic-embed-s` (384-dim, retrieval-optimized). See [tradeoffs/03-embeddings.md](tradeoffs/03-embeddings.md).

### Vector Store: [Qdrant](https://qdrant.tech/)

**Motivation:** Company Intelligence queries are entity-heavy ("Who is the CEO?", "When was Series B?"). Pure vector search misses exact-term matches. Qdrant provides hybrid search (BM25 + dense vectors via RRF fusion) and pre-search filtering — both critical for retrieval quality. First-party Aspire integration avoids container hacks.

**What it proves:** Hybrid search improves retrieval for entity-heavy domains. Pre-search filtering gives correct results when scoping to a specific company.

**Previously:** ChromaDB. Switched for hybrid search and Aspire integration. See [tradeoffs/04-vector-store.md](tradeoffs/04-vector-store.md).

### Web Scraping: [Crawl4AI](https://github.com/unclecode/crawl4ai)

**Motivation:** The gather phase needs to scrape company websites, Wikipedia, and news. Crawl4AI outputs clean Markdown natively (token-efficient for embeddings), handles JS-rendered pages via Playwright, and removes boilerplate. This is the most LLM-pipeline-friendly scraper available.

**What it proves:** Automated scraping can produce clean enough Markdown for direct chunking without a custom HTML→text pipeline.

### Orchestration: [.NET Aspire](https://aspire.dev/)

**Motivation:** The POC has 3+ services (agent, Ollama, Qdrant) that need to start together, discover each other, and report health. Aspire provides single `dotnet run` startup, service discovery, health checks, and a built-in OTLP dashboard — all without writing Docker Compose or Kubernetes manifests.

**What it proves:** Polyglot orchestration (Python agent + .NET host + Docker containers) works with minimal config. Aspire dashboard replaces the need for a separate Grafana/Jaeger stack.

### Backend Framework: [Pydantic AI](https://ai.pydantic.dev/) + [FastAPI](https://fastapi.tiangolo.com/)

**Motivation:** Pydantic AI provides typed agent tools, structured outputs, and built-in AG-UI adapter (`AGUIApp`). FastAPI serves as the ASGI host with SSE streaming. Together they give us a production-grade agent backend with minimal code.

**What it proves:** A typed, observable RAG agent can be built with ~200 lines of Python, not a framework-heavy scaffold.

## 2. Production-Grade Practices in POC Scope

Each practice is included because it's cheap to adopt now and expensive to retrofit later.

| Practice                       | How We Apply It                                                | Why in POC                                                                                       |
| ------------------------------ | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |
| **Observability from day one** | OTel traces + GenAI conventions → Aspire Dashboard             | Debugging RAG failures without traces is guesswork. Cost to add now: ~10 lines of setup          |
| **Structured orchestration**   | Aspire AppHost: service discovery, health checks, unified logs | Alternative is a shell script with `docker run` commands. Aspire is barely more code             |
| **Typed contracts**            | Pydantic models for RawDocument, Chunk, ChunkMetadata          | Catches schema errors at parse time, not at query time. Free with Pydantic                       |
| **Source traceability**        | Every chunk tracks origin URL + scrape timestamp               | Required for citations. Must be designed in from the start — can't retrofit metadata             |
| **Offline-first**              | Local LLM + local embeddings + local vector store              | Spec requirement. Drives all technology choices                                                  |
| **Separation of concerns**     | Gather phase (online) vs. Query phase (offline)                | Clean boundary. Gather can be re-run without affecting query. Independent testing                |
| **Modular architecture**       | Swap LLM, vector store, or scraper independently               | Already proven: switched ChromaDB→Qdrant, MiniLM→arctic-embed, llama3→qwen3 with minimal changes |
| **Citation in answers**        | Every response references source URL                           | Core requirement. Metadata design must support this from chunk creation onward                   |

## 3. Explicitly Deferred

Not in POC scope. Documented so we don't accidentally scope-creep.

| Concern                                   | Why Deferred                                          |
| ----------------------------------------- | ----------------------------------------------------- |
| Custom frontend                           | CopilotKit covers POC needs                           |
| Incremental scraping (content hash + TTL) | Fresh gather is acceptable for 5-20 companies         |
| Multi-tenant isolation                    | Single-user POC                                       |
| Authentication / authorization            | Local-only tool                                       |
| CI/CD pipeline                            | Manual `dotnet run` is fine for POC                   |
| Reranker (cross-encoder)                  | Add when retrieval quality is measurably insufficient |
| Conversation persistence                  | In-memory is acceptable for POC                       |
